\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1.0in]{geometry}
\title{Reinforcement Learning Algorithms used in Tic Tac Toe}
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 										TD(0)											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TD(0) Learning}

\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 									& \text{with probability } \epsilon \\
\text{argmax}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for white} \\
\text{argmin}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Store the transition $(s_t, s_{t+1}, r_t, a_t)$ in the replay buffer $D$ \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Set the TD-target: 
$
y_t= 
\begin{cases}
	r_t 										& \text{if game terminates at step } t+1 \\
	r_{t} + \gamma \tilde{V}(s_{t+1}, \theta)   & \text{otherwise} \\
\end{cases}
$ \\
\indent \indent Perform a stochastic gradient decent step on $[y_t - V(s_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps set $\tilde{V} = V$   \\
\indent  \textbf {End For}  \\
\textbf {End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 									DQN(lambda)											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DQN($\lambda$)}
\textbf{procedure} REFRESH($l$) \\
\indent \textbf{For} transition $(s_t, s_{t+1}, r_t, R_t^\lambda, a_t)$ $\in l$ processing back-to-front \textbf{Do} \\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent Update $R_t^\lambda \leftarrow r_t + \gamma [\gamma R_{t+1}^\lambda + (1 - \lambda) V(s_{t+1}, \theta)]$ \\
\indent \indent \textbf{Else} \\
\indent \indent \indent Get adjacent transition $(s_{t+1}, s_{t+2}, r_{t+1}, R_{t+1}^\lambda, a_{t+1})$ from $l$ \\
\indent \indent \textbf{End If} \\
\indent \textbf{End For} \\
\textbf{End procedure} \\
\\
\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 									& \text{with probability } \epsilon \\
\text{argmax}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for white} \\
\text{argmin}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Append the transition $(s_t, s_{t+1}, r_t,  R_t^\lambda, a_t)$ to $L$, where $R_t^\lambda$ is arbitrary\\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent REFRESH($L$) \\
\indent \indent \indent Store $L$ in D \\
\indent \indent \textbf{End If} \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Perform a stochastic gradient decent step on $[R_t^\lambda - V(s_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps REFRESH($D$)   \\
\indent  \textbf{End For}  \\
\textbf{End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak



\end{document}
