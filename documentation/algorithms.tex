\documentclass{article}
\usepackage{amsmath}
\usepackage[margin=1.0in]{geometry}
\title{Reinforcement Learning Algorithms used in Tic Tac Toe}
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 										TD(0)											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TD(0) Learning}

\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 									& \text{with probability } \epsilon \\
\text{argmax}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for white} \\
\text{argmin}_a \tilde{V}(succ(s_t, a), \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Store the transition $(s_t, s_{t+1}, r_t, a_t)$ in the replay buffer $D$ \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Set the TD-target: 
$
y_t= 
\begin{cases}
	r_t 										& \text{if game terminates at step } t+1 \\
	r_{t} + \gamma \tilde{V}(s_{t+1}, \theta)   & \text{otherwise} \\
\end{cases}
$ \\
\indent \indent Perform a stochastic gradient decent step on $[y_t - V(s_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps set $\tilde{V} = V$   \\
\indent  \textbf {End For}  \\
\textbf {End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 									TD(lambda)											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TD($\lambda$)}
\textbf{procedure} REFRESH($l$) \\
\indent \textbf{For} transition $(s_t, s_{t+1}, r_t, R_t^\lambda, a_t)$ $\in l$ processing back-to-front \textbf{Do} \\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent Update $R_t^\lambda \leftarrow r_t$ \\
\indent \indent \textbf{Else} \\
\indent \indent \indent Get adjacent transition $(s_{t+1}, s_{t+2}, r_{t+1}, R_{t+1}^\lambda, a_{t+1})$ from $l$ \\
\indent \indent \indent Update $R_t^\lambda \leftarrow r_t + \gamma [\gamma R_{t+1}^\lambda + (1 - \lambda) V(s_{t+1}, \theta)]$ \\
\indent \indent \textbf{End If} \\
\indent \textbf{End For} \\
\textbf{End procedure} \\
\\
\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 							& \text{with probability } \epsilon \\
\text{argmax}_a V(succ(s_t, a), \theta)     & \text{for white} \\
\text{argmin}_a V(succ(s_t, a), \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Append the transition $(s_t, s_{t+1}, r_t,  R_t^\lambda, a_t)$ to $L$, where $R_t^\lambda$ is arbitrary\\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent REFRESH($L$) \\
\indent \indent \indent Store $L$ in D \\
\indent \indent \textbf{End If} \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Perform a stochastic gradient decent step on $[R_t^\lambda - V(s_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps REFRESH($D$)   \\
\indent  \textbf{End For}  \\
\textbf{End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 										Q-Learning										  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Q-Learning}

\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 							  & \text{with probability } \epsilon \\
\text{argmax}_a \tilde{Q}(s_t, a, \theta)     & \text{for white} \\
\text{argmin}_a \tilde{Q}(s_t, a, \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Store the transition $(s_t, s_{t+1}, r_t, a_t)$ in the replay buffer $D$ \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Set the target: 
$
y_t= 
\begin{cases}
r_t 										& \text{if game terminates at step } t+1 \\
r_{t} + \gamma \text{max}_{a'}\tilde{Q}(s_{t+1}, a', \theta)   & \text{otherwise} \\
\end{cases}
$ \\
\indent \indent Perform a stochastic gradient decent step on $[y_t - Q(s_t, a_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps set $\tilde{Q} = Q$   \\
\indent  \textbf {End For}  \\
\textbf {End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 									DQN(lambda)											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{DQN($\lambda$)}
\textbf{procedure} REFRESH($l$) \\
\indent \textbf{For} transition $(s_t, s_{t+1}, r_t, R_t^\lambda, a_t)$ $\in l$ processing back-to-front \textbf{Do} \\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent Update $R_t^\lambda \leftarrow r_t$ \\
\indent \indent \textbf{Else} \\
\indent \indent \indent Get adjacent transition $(s_{t+1}, s_{t+2}, r_{t+1}, R_{t+1}^\lambda, a_{t+1})$ from $l$ \\
\indent \indent \indent Update $R_t^\lambda \leftarrow r_t + \gamma [\gamma R_{t+1}^\lambda + (1 - \lambda) \text{max}_{a'}Q(s_{t+1}, a', \theta)]$ \\
\indent \indent \textbf{End If} \\
\indent \textbf{End For} \\
\textbf{End procedure} \\
\\
\textbf{For} episode = 1, M \textbf{do} \\
\indent Initialize a fresh game \\
\indent \textbf {For} t = 1, T \textbf{do} \\
\indent \indent Play move
$
a_t= 
\begin{cases}
\text{random move} 									& \text{with probability } \epsilon \\
\text{argmax}_a Q(s_t, a, \theta)     & \text{for white} \\
\text{argmin}_a Q(s_t, a, \theta)     & \text{for black}
\end{cases}
$ \\
\indent \indent Receive reward $r_t$  \\
\indent \indent Append the transition $(s_t, s_{t+1}, r_t,  R_t^\lambda, a_t)$ to $L$, where $R_t^\lambda$ is arbitrary\\
\indent \indent \textbf{If} terminal($s_{t+1}$) \textbf{Then} \\
\indent \indent \indent REFRESH($L$) \\
\indent \indent \indent Store $L$ in D \\
\indent \indent \textbf{End If} \\
\indent \indent Sample a random minibatch transition from $D$  \\
\indent \indent Perform a stochastic gradient decent step on $[R_t^\lambda - Q(s_t, a_t, \theta)]^2$ with respect to $\theta$  \\
\indent \indent Every $c$ steps REFRESH($D$)   \\
\indent  \textbf{End For}  \\
\textbf{End For} \\

\noindent $\gamma$ is the discount factor and $\theta$ the neural network parameters.

\pagebreak




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 									AlphaZero											  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{AlphaZero}
\subsection{Monte-Carlo Tree Search (MCTS)}
\subsubsection{Upper Confidence Bound}
$ U(s,a) = Q(s,a) + \sqrt{\frac{2\ln{\sum\nolimits_{b}N(s,b)}}{1 + N(s,a)}}$ \\

\noindent
$U(s,a)$ is the upper confidence bound for the current state $s$ and action $a$ \\
$Q(s,a)$ is the expected reward by taking action $a$ in state $s$ \\
$N(s,a)$ is the number of times we took action $a$ from state $s$ \\
$\sum\nolimits_{b}N(s,b)$ is the total number of plays from state $s$ \\
\\


\subsubsection{Upper Confidence Bound Alpha Zero}
\noindent 
$ U(s,a) = Q(s,a) + c_{puct} P(s,a) \frac{\sqrt{\sum\nolimits_{b}N(s,b)}}{1 + N(s,a)}$ \\

\noindent 
$U(s,a)$ is the upper confidence bound for the current state $s$ and action $a$. \\
$Q(s,a)$ is the expected reward by taking action $a$ in state $s$. \\
$c_{puct}$ is a constant that controls the amount exploration \\ 
$P(s,a)$ probability to take action $a$ in state $s$ as predicted by the neural network \\
$N(s,a)$ is the number of times we took action $a$ from state $s$ \\
$\sum\nolimits_{b}N(s,b)$ is the total number of plays from state $s$ \\
\\
\pagebreak


\subsubsection{Alpha Zero Tree Search}
\textbf{procedure} SEARCH($s$) \\
\indent \textbf{If} terminal($s_t$) \textbf{Then} \\
\indent \indent \textbf{Return} $r_t$ \\
\indent \textbf{End If} \\
\\
\indent \textbf{If not} exists($P(s, .)$) \textbf{Then} \\
\indent \indent predict $P(s, .)$ and $v(s)$ with the neural network \\
\indent \indent $N_s(s) = 0$ \\
\indent \indent $Q(s, a) = 0$ for all $a$ \\
\indent \indent $N(s, a) = 0$ for all $a$ \\
\indent \indent \textbf{Return} $v(s)$ \\
\indent \textbf{End If} \\
\\
\indent $U(s,a) = Q(s,a) + c_{puct} P(s,a) \frac{\sqrt{N_s(s)}}{1 + N(s,a)}$ for all $a$ \\
\indent $a_t = \text{argmax}_a U(s,a)$ \\
\indent Execute $a_t$ to get next state $s_{t+1}$ \\
\indent $v(s_{t+1}) = \text{SEARCH}(s_{t+1})$ \\
\\
\indent \textbf{If} player == BLACK \textbf{Then} \\
\indent \indent $v = -v(s_{t+1})$ \\
\indent \textbf{Else} \\
\indent \indent $v = v(s_{t+1})$ \\
\indent \textbf{End If} \\
\\
\indent $Q(s,a) = \frac{N(s,a) Q(s,a) + v}{N(s,a) + 1}$ \\
\indent $N(s,a) = N(s,a) + 1$ \\
\indent $N_s(s) = N_s(s) + 1$ \\
\indent \textbf{Return} v \\
\textbf{End procedure} \\
\\
\textbf{procedure} MCTSAZ($s$) \\
\indent \textbf{For} simulation = 1, M \textbf{Do} \\
\indent \indent SEARCH($s_t$) \\
\indent \textbf{End} \\
\indent \textbf{Return} $N(s,a)$\\
\textbf{End procedure} \\


\pagebreak
\subsubsection{Training Algorithm}
\textbf{For} episode = 1, M \textbf{Do} \\
\indent \textbf{For} t = 1, T \textbf{Do} \\
\indent \indent Initialize $N_s$, $N$, $Q$, $U$ and $P$ \\
\indent \indent Initialize a fresh game \\
\indent \indent $N(s_t,a) = \text{MCTSAZ}(s)$ \\
\indent \indent \textbf{If} $temp == 0$ \textbf{Then} \\
\indent \indent \indent $a_t = \text{argmax}_a N(s_,a)$ \\
\indent \indent \indent $P(s_t,a) = 
\begin{cases}
	1		& \text{for } a_t \\
	0    	& \text{otherwise} \\
\end{cases}
$ \\
\indent \indent \textbf{Else} \\
\indent \indent \indent $P(s_t,a) = N(s,a)^{\frac{1}{temp}}$ \\
\indent \indent \indent $P(s_t,a) = \frac{P(s,a)}{\sum\nolimits_{b}P(s_t,b)}$ \\
\indent \indent \textbf{End If} \\
\\
\indent \indent Append the training example $(s_t, P(s_t,a), v_t)$ to $L$, where $v_t$ is arbitrary \\
\indent \indent Pick action $a_t$ by sampling from $P(s_t,a)$ \\
\indent \indent Play move $a_t$ \\
\indent \textbf{End} \\
\\
\indent Observe the final reward $r_T$ of the game \\
\indent \textbf{For} training example $(s_t, P(s_t,a), v_t) \in L$ \textbf{Do} \\
\indent \indent \textbf{If} player == WHITE \textbf{Then} \\
\indent \indent \indent Update $v_t \leftarrow r_T$ \\
\indent \indent \textbf{Else} \\
\indent \indent \indent Update $v_t \leftarrow -r_T$ \\
\indent \indent \textbf{End If} \\
\indent \textbf{End} \\
\textbf{End} \\



\pagebreak



\end{document}
